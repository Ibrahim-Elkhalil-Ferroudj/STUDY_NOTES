{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2215a2c",
   "metadata": {},
   "source": [
    "# Data Cleaning and preprocessing \n",
    "\n",
    "***What is data cleaning?*** \n",
    "This involves identifying and correcting or removing errors and inconsistencies in your dataset. Errors can come in many forms, i.e. missing values, duplicated records or outliers. \n",
    "If left unchecked, these problems can lead to inaccurate models and unreliable predictions. \n",
    "\n",
    "***Data Preprocessing***: While data cleaning is about fixing issues with the data, preprocessing is about transforming it into a format that is suitable for analysis and model training. \n",
    "This might involve normalizing, standardizing, encoding categorical variables or splitting the data into training and testing sets. \n",
    "\n",
    "***Why is this important?*** \n",
    "Data cleaning and preprocessing allows the model to learn the underlying patterns more effectively leading to better predictions and ultimately better performance.\n",
    "\n",
    "## Managing missing values\n",
    "Missing values are a commong issure in data sets and can arise for various raisons, such as data entry errors or unavailability of certain information. If not addressed, missing values can lead to biased results or reduce the accuracy of the model. \n",
    "\n",
    "### Strategies for handling missing values\n",
    "1. Removing missing data\n",
    "If a small number of rows or columns have missing values. You might consider removing them from the data set. This approach is useful when the missing data is minimal and its removal won't significantly impact the dataset.\n",
    "**Code Example** \n",
    "```python\n",
    "#Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Drop columns with missing values\n",
    "df_cleaned = df.dropna(axis = 1)\n",
    "```\n",
    "2. Impute missing Data\n",
    "Imputation involves filling in missing values with a substitute value, such as the mean, median or mode of the columns. This is useful when missing data is more prevalent but you don't want to lose information by removing rows or columns.\n",
    "\n",
    "**Code Example**: \n",
    "```python\n",
    "# Fill missing values with the mean of the column\n",
    "df['Column_name'].fillna(df['Column_name'].mean(), inplace = True)\n",
    "\n",
    "# Fill missing values with the median\n",
    "df['Column_name'].fillna(df['Column_name'].median(), inplace = True)\n",
    "```\n",
    "3. Forward or Backward fill\n",
    "Forward fill propagates the last valid observation forward, while backward fill does the opposite. This is particularly useful in time serie data where tranch or sequence are important.\n",
    "\n",
    "**Code Example**:\n",
    "\n",
    "```python\n",
    "# Forward fill\n",
    "df.fillna(method = 'ffil', inplace = True)\n",
    "\n",
    "# Backward fill\n",
    "df.fillna(method = 'bfil', inplace = True)\n",
    "```\n",
    "\n",
    "## Manage Outliers\n",
    "1. Identify Outliers \n",
    "The first step is identifying outliers, which can be done using statistical methods such as z-score or inter quartile range *(IQR)*\n",
    "**Code Example**: \n",
    "```python\n",
    "\n",
    "#Using z-score \n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z_scores = np.abs(stats.zscore(df['Column_name']))\n",
    "outliers = df[z_scores > 3]\n",
    "\n",
    "# using IQR \n",
    "\n",
    "Q1 = df['Column_name'].quantile(0.25)\n",
    "Q3 = df['Column_name'].quantile(0.45)\n",
    "Iqr = Q3-Q1\n",
    "outliers = df[df(['Column_name'] < (Q1-1.5*Iqr1)) | (df['Column_name']> (Q3 + 1.5*Iqr))]\n",
    "```\n",
    "2. Handle outliers\n",
    "- Remove outliers : They can be removed from the dataset if they are believed to be errors or not representative of the population.\n",
    "\n",
    "**Code Example** \n",
    "```python\n",
    "# Remove outliers identifyed by z-score\n",
    "df_cleaned = df[(z_scores<=3)]\n",
    "\n",
    "# Remove outliers identified by IQR\n",
    "df_cleaned = df[~((df['Column_name'] < (Q1-1.5*Iqr))| (df['Column_namr'] < (Q1 + 1.5*Iqr)))]\n",
    "```\n",
    "\n",
    "- Cap or transform outliers: Instead of removing outliers, you might cap them into a certain thershold or transform them using logarithmic or other funcitons to reduce their impact. \n",
    "\n",
    "**Code Example**: \n",
    "```python\n",
    "# Cap outliers to a threshold\n",
    "df['Column_name'] = np.where(df['Column_name'] > upper_threshold , upper_threshold, df['Column_name'])\n",
    "\n",
    "# Logarithmic transform to reduce the impace \n",
    "df['Column_name'] = np.log(df['Column_name'] + 1)\n",
    "```\n",
    "\n",
    "\n",
    "## Normalization\n",
    "\n",
    "***Normalization (or scaling)*** is the process of adjusting the values of numeric columns in a dataset to a common scale, typically between 0-1. This is especially important for Machine Learning algorithms that rely on the magnitude of features such as gradient descent based algorithms.\n",
    "\n",
    "### Methods for normalization \n",
    "1. Min-Max scaling: scales all numeric in a column between 0-1. \n",
    "*Example* : \n",
    "```python\n",
    "from Sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df['Scaled_Column'] = scaler.fit_transform(df[['Column_name']])\n",
    "```\n",
    "2. Z-Score standardization: Scales the data so that it has **Mean = 0, Std=1**. This is useful when you want to compare features with different units or scaler. \n",
    "\n",
    "```python\n",
    "from Sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df['Standardized_Column'] = scaler.fit_transform(df[['Column_Name']])\n",
    "```\n",
    "\n",
    "## Data Transformation\n",
    "This involves converting data from one format or structure to another. This is often necessary to meet the assumptions of statistical mmodels or to improve the performance of Machine learning algorithms. \n",
    "\n",
    "### Common data transformations \n",
    "1. Logarithmic transformation: This is used to stabilize variance, by making the data appear more like normal distribution and reducing the impact of outliers *e.g* `df['log_column'] = np.log(df['Column_name'] + 1)`\n",
    "\n",
    "2. Box-Cox transformation : This is used to stabilize variance and make the data more normally distributed. \n",
    "*Example* : \n",
    "```python\n",
    "from scipy import stats\n",
    "df['boxcox_column'],_ = stats.boxcox(df['Column_name'] + 1)\n",
    "```\n",
    "\n",
    "3. Binning (or *Discretiation*) : This involves converting continous variables into discrete categories\n",
    "*Example*\n",
    "```python\n",
    "# Creating bins for a continous variable \n",
    "df['binned_column'] = pd.cut(df['Column_name'], bins = [0,10,20,30], labels= 'Low', 'Medium', 'High')\n",
    "```\n",
    "\n",
    "4. Encoding Categorical variables : This is transforming categorical data into numerical format, which is necessary for many machine learning algorithms.\n",
    "*One Hot coding* : `df_encoded = pd.get_dummies(df,columns = ['category '])`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
