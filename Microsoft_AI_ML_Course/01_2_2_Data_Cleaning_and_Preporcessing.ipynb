{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2215a2c",
   "metadata": {},
   "source": [
    "# Data Cleaning and preprocessing \n",
    "\n",
    "***What is data cleaning?*** \n",
    "This involves identifying and correcting or removing errors and inconsistencies in your dataset. Errors can come in many forms, i.e. missing values, duplicated records or outliers. \n",
    "If left unchecked, these problems can lead to inaccurate models and unreliable predictions. \n",
    "\n",
    "***Data Preprocessing***: While data cleaning is about fixing issues with the data, preprocessing is about transforming it into a format that is suitable for analysis and model training. \n",
    "This might involve normalizing, standardizing, encoding categorical variables or splitting the data into training and testing sets. \n",
    "\n",
    "***Why is this important?*** \n",
    "Data cleaning and preprocessing allows the model to learn the underlying patterns more effectively leading to better predictions and ultimately better performance.\n",
    "\n",
    "## Managing missing values\n",
    "Missing values are a commong issure in data sets and can arise for various raisons, such as data entry errors or unavailability of certain information. If not addressed, missing values can lead to biased results or reduce the accuracy of the model. \n",
    "\n",
    "### Strategies for handling missing values\n",
    "1. Removing missing data\n",
    "If a small number of rows or columns have missing values. You might consider removing them from the data set. This approach is useful when the missing data is minimal and its removal won't significantly impact the dataset.\n",
    "**Code Example** \n",
    "```python\n",
    "#Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Drop columns with missing values\n",
    "df_cleaned = df.dropna(axis = 1)\n",
    "```\n",
    "2. Impute missing Data\n",
    "Imputation involves filling in missing values with a substitute value, such as the mean, median or mode of the columns. This is useful when missing data is more prevalent but you don't want to lose information by removing rows or columns.\n",
    "\n",
    "**Code Example**: \n",
    "```python\n",
    "# Fill missing values with the mean of the column\n",
    "df['Column_name'].fillna(df['Column_name'].mean(), inplace = True)\n",
    "\n",
    "# Fill missing values with the median\n",
    "df['Column_name'].fillna(df['Column_name'].median(), inplace = True)\n",
    "```\n",
    "3. Forward or Backward fill\n",
    "Forward fill propagates the last valid observation forward, while backward fill does the opposite. This is particularly useful in time serie data where tranch or sequence are important.\n",
    "\n",
    "**Code Example**:\n",
    "\n",
    "```python\n",
    "# Forward fill\n",
    "df.fillna(method = 'ffil', inplace = True)\n",
    "\n",
    "# Backward fill\n",
    "df.fillna(method = 'bfil', inplace = True)\n",
    "```\n",
    "\n",
    "## Manage Outliers\n",
    "1. Identify Outliers \n",
    "The first step is identifying outliers, which can be done using statistical methods such as z-score or inter quartile range *(IQR)*\n",
    "**Code Example**: \n",
    "```python\n",
    "\n",
    "#Using z-score \n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z_scores = np.abs(stats.zscore(df['Column_name']))\n",
    "outliers = df[z_scores > 3]\n",
    "\n",
    "# using IQR \n",
    "\n",
    "Q1 = df['Column_name'].quantile(0.25)\n",
    "Q3 = df['Column_name'].quantile(0.45)\n",
    "Iqr = Q3-Q1\n",
    "outliers = df[df(['Column_name'] < (Q1-1.5*Iqr1)) | (df['Column_name']> (Q3 + 1.5*Iqr))]\n",
    "```\n",
    "2. Handle outliers\n",
    "- Remove outliers : They can be removed from the dataset if they are believed to be errors or not representative of the population.\n",
    "\n",
    "**Code Example** \n",
    "```python\n",
    "# Remove outliers identifyed by z-score\n",
    "df_cleaned = df[(z_scores<=3)]\n",
    "\n",
    "# Remove outliers identified by IQR\n",
    "df_cleaned = df[~((df['Column_name'] < (Q1-1.5*Iqr))| (df['Column_namr'] < (Q1 + 1.5*Iqr)))]\n",
    "```\n",
    "\n",
    "- Cap or transform outliers: Instead of removing outliers, you might cap them into a certain thershold or transform them using logarithmic or other funcitons to reduce their impact. \n",
    "\n",
    "**Code Example**: \n",
    "```python\n",
    "# Cap outliers to a threshold\n",
    "df['Column_name'] = np.where(df['Column_name'] > upper_threshold , upper_threshold, df['Column_name'])\n",
    "\n",
    "# Logarithmic transform to reduce the impace \n",
    "df['Column_name'] = np.log(df['Column_name'] + 1)\n",
    "```\n",
    "\n",
    "\n",
    "## Normalization\n",
    "\n",
    "***Normalization (or scaling)*** is the process of adjusting the values of numeric columns in a dataset to a common scale, typically between 0-1. This is especially important for Machine Learning algorithms that rely on the magnitude of features such as gradient descent based algorithms.\n",
    "\n",
    "### Methods for normalization \n",
    "1. Min-Max scaling: scales all numeric in a column between 0-1. \n",
    "*Example* : \n",
    "```python\n",
    "from Sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df['Scaled_Column'] = scaler.fit_transform(df[['Column_name']])\n",
    "```\n",
    "2. Z-Score standardization: Scales the data so that it has **Mean = 0, Std=1**. This is useful when you want to compare features with different units or scaler. \n",
    "\n",
    "```python\n",
    "from Sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df['Standardized_Column'] = scaler.fit_transform(df[['Column_Name']])\n",
    "```\n",
    "\n",
    "## Data Transformation\n",
    "This involves converting data from one format or structure to another. This is often necessary to meet the assumptions of statistical mmodels or to improve the performance of Machine learning algorithms. \n",
    "\n",
    "### Common data transformations \n",
    "1. Logarithmic transformation: This is used to stabilize variance, by making the data appear more like normal distribution and reducing the impact of outliers *e.g* `df['log_column'] = np.log(df['Column_name'] + 1)`\n",
    "\n",
    "2. Box-Cox transformation : This is used to stabilize variance and make the data more normally distributed. \n",
    "*Example* : \n",
    "```python\n",
    "from scipy import stats\n",
    "df['boxcox_column'],_ = stats.boxcox(df['Column_name'] + 1)\n",
    "```\n",
    "\n",
    "3. Binning (or *Discretiation*) : This involves converting continous variables into discrete categories\n",
    "*Example*\n",
    "```python\n",
    "# Creating bins for a continous variable \n",
    "df['binned_column'] = pd.cut(df['Column_name'], bins = [0,10,20,30], labels= 'Low', 'Medium', 'High')\n",
    "```\n",
    "\n",
    "4. Encoding Categorical variables : This is transforming categorical data into numerical format, which is necessary for many machine learning algorithms.\n",
    "*One Hot coding* : `df_encoded = pd.get_dummies(df,columns = ['category '])`\n",
    "\n",
    "## Practice activity, setup a local data cleaning and preprocessing tool instruction\n",
    "\n",
    "***Note***: Key python libraries for data cleaning and preprocessing. \n",
    "- Pandas : for data manipulation and analysis. \n",
    "- Numpy : For numerical operations. \n",
    "- Scikit-Learn : for data preprocessing and machine learning tasks.\n",
    "- Missingno : for visualizing missing data \n",
    "\n",
    "### Creating the script \n",
    "#### Step 1 \n",
    "***importing the required libraries.*** \n",
    "```python \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "```\n",
    "#### Step 2\n",
    "***load the dataset***\n",
    "```python \n",
    "df = pd.read_csv('Your_dataset.csv')\n",
    "print(df.head())\n",
    "```\n",
    "#### Step 3 \n",
    "***Handle missing data***, where you can choose whether to remove, fill or visualize the missing data\n",
    "```python\n",
    "# Visualization of missing data\n",
    "msno.matrix(df)\n",
    "msno.heatmap(df)\n",
    "\n",
    "df_cleaned = df.dropna() # Dropping rows with missing values\n",
    "df_filled = df.fillna(df.mean()) # filling missing values with mean\n",
    "```\n",
    "\n",
    "#### Step 4\n",
    "***Handle Outliers***\n",
    "```python\n",
    "from scipy import stats\n",
    "# Using z scores\n",
    "z_scores = np.abs(stats.zscore(df_cleaned))\n",
    "df_no_outliers = df_cleaned[(z_scores < 3 ).all(axis = 1)]\n",
    "\n",
    "# Or cap outliers at a threshold \n",
    "upper_limit = df_cleaned['Column_name'].quantile(0.95)\n",
    "df_cleaned['Column_name'] = np.where(df_cleaned['Column_name'] > upper_limit, upper_limit, df_cleaned['Column_name'])\n",
    "```\n",
    "#### Step 5 \n",
    "***Scale and normalize the data***, to ensure that all features contribute equally to the model. \n",
    "\n",
    "```python\n",
    "# MinMax scalling \n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_cleaned), columns = df_cleaned.columns)\n",
    "\n",
    "# z_score normalization\n",
    "scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df_cleaned), columns= df_cleaned.columns)\n",
    "```\n",
    "\n",
    "#### Step 6\n",
    "***Encode categorized variables***\n",
    "```python\n",
    "# One Hot encoding for categorical variables\n",
    "df_encoded = pd.get_dummies(df_scaled, columns = ['Categorical_Column_Name'])\n",
    "```\n",
    "\n",
    "#### Step 7 \n",
    "***Save the cleaned and preprocessed data***\n",
    "```python\n",
    "df_encoded.to_csv('Cleaned_preprocessed_data.csv', index = False)\n",
    "```\n",
    "\n",
    "##### Automate the workflow \n",
    "```python\n",
    "def load_data(filepath): \n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def handle_missing_value(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "def remove_outliers(df): \n",
    "    z_scores = np.abs(stats.zscore(df))\n",
    "    return df[(z_scores<3).all(axis = 1)]\n",
    "\n",
    "def scale_data(df): \n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "def encode_categorical(df, categorical_columns): \n",
    "    return pd.get_dummies(df,columns = categorical_columns)\n",
    "\n",
    "def save_data(df, output_file_path): \n",
    "    return df.to_csv(output_file_path, index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d25e0",
   "metadata": {},
   "source": [
    "### Practice on dummy data \n",
    "some typically encountered data will be generated and handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc968ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     Feature1  Feature2 Category  Target\n",
      "0   97.707097        82        A       1\n",
      "1  115.765726        68        B       1\n",
      "2   94.719286        27        C       1\n",
      "3   91.771499        15        D       0\n",
      "4  118.270547        58        A       0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.to_csv(output_file_path, index = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Handling missing values\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m df = \u001b[43mhandle_missing_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Removing outliers\u001b[39;00m\n\u001b[32m     41\u001b[39m df = remove_outliers(df.select_dtypes(include=[np.number]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mhandle_missing_value\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_missing_value\u001b[39m(df):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.fillna(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:11720\u001b[39m, in \u001b[36mDataFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11712\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11713\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  11714\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11718\u001b[39m     **kwargs,\n\u001b[32m  11719\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11720\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11721\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11722\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:12485\u001b[39m, in \u001b[36mNDFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  12479\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12480\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12483\u001b[39m     **kwargs,\n\u001b[32m  12484\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12486\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:12442\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12438\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12440\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:11589\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11585\u001b[39m     df = df.T\n\u001b[32m  11587\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11588\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11589\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11590\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1519\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1517\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1518\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1522\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:406\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    409\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:11508\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11506\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\nanops.py:719\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    716\u001b[39m     dtype_count = dtype\n\u001b[32m    718\u001b[39m count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m the_sum = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m the_sum = _ensure_numeric(the_sum)\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:51\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Creating a dummy dataset\n",
    "data = {'Feature1': np.random.normal(100,10,100).tolist() + [np.nan,20], # Numerical with NaN and outlier\n",
    "        'Feature2': np.random.randint(0,100,102).tolist(), # Numerical\n",
    "        'Category': ['A', 'B', 'C', 'D']*25 + [np.nan, 'A'], # Categorical with NaN\n",
    "        'Target': np.random.choice([0,1],102).tolist()} # Target variable\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Displaying the first few rows of the dataset\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Applying the preprocessing tools \n",
    "def handle_missing_value(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "def remove_outliers(df): \n",
    "    z_scores = np.abs(stats.zscore(df))\n",
    "    return df[(z_scores<3).all(axis = 1)]\n",
    "\n",
    "def scale_data(df): \n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "def encode_categorical(df, categorical_columns): \n",
    "    return pd.get_dummies(df,columns = categorical_columns)\n",
    "\n",
    "def save_data(df, output_file_path): \n",
    "    return df.to_csv(output_file_path, index = False)\n",
    "\n",
    "# Handling missing values\n",
    "df = handle_missing_value(df)\n",
    "\n",
    "# Removing outliers\n",
    "df = remove_outliers(df.select_dtypes(include=[np.number]))\n",
    "df = df.join(df.select_dtypes(exclude=[np.number]))\n",
    "\n",
    "# Scaling numerical data\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "df[numerical_columns] = scale_data(df[numerical_columns])\n",
    "\n",
    "# Encoding categorical data\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "df = encode_categorical(df, categorical_columns)\n",
    "\n",
    "# Saving the processed data\n",
    "output_file_path = 'processed_data.csv'\n",
    "save_data(df, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
